{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集目录结构\n",
    "```\n",
    "FashionAI-Attributes\n",
    "├── models/\n",
    "├── data/\n",
    "│   ├── base/\n",
    "│   └── rank/\n",
    "└── solution.ipynb\n",
    "```\n",
    "\n",
    "python package依赖\n",
    "```\n",
    "matplotlib\n",
    "numpy\n",
    "pandas\n",
    "tensorflow\n",
    "keras == 2.1.4\n",
    "pillow\n",
    "h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置训练任务\n",
    "本次比赛训练八个不同任务图片分类器，要想处理其他类型的服饰，只需要将task变量指定为对应的服饰类型名称就行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['collar_design_labels', 'neckline_design_labels', 'skirt_length_labels', \n",
    "           'sleeve_length_labels', 'neck_design_labels', 'coat_length_labels', 'lapel_design_labels', \n",
    "           'pant_length_labels']\n",
    "task = classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看图片数据示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"data/base/Images/\", task)\n",
    "\n",
    "first_image_path = os.path.join(data_path, os.listdir(data_path)[0])\n",
    "img = plt.imread(first_image_path)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据整理\n",
    "Keras提供了一个读取图像分类任务数据的接口keras.preprocessing.image.ImageDataGenerator，它期望数据是类似ImageNet的格式组织的，即每一类图片都统一放在一个目录下。因此我们需要将数据进行整理，并且取10%的数据作为验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(path):\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))\n",
    "mkdir_if_not_exist(['data/base/train_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = 'data/base/Annotations/label.csv'\n",
    "\n",
    "image_path = []\n",
    "\n",
    "with open(label_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    for path, tk, label in tokens:\n",
    "        if tk == task:\n",
    "            image_path.append(('data/base/' + path, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_class = len(image_path[0][1])\n",
    "print(image_path[0])\n",
    "print(len(image_path))\n",
    "print(task_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练集和测试集数据目录，以及类别的子目录\n",
    "假设当前分类任务为collar_design_labels，则运行后的目录结构如下:\n",
    "```\n",
    "train_valid\n",
    "└── collar_design_labels\n",
    "    ├── train\n",
    "    │   ├── 0\n",
    "    │   ├── 1\n",
    "    │   ├── 2\n",
    "    │   ├── 3\n",
    "    │   └── 4\n",
    "    └── val\n",
    "        ├── 0\n",
    "        ├── 1\n",
    "        ├── 2\n",
    "        ├── 3\n",
    "        └── 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_if_not_exist(['data/train_valid', task])\n",
    "mkdir_if_not_exist(['data/train_valid', task, 'train'])\n",
    "mkdir_if_not_exist(['data/train_valid', task, 'valid'])\n",
    "for i in range(task_class):\n",
    "    mkdir_if_not_exist(['data/train_valid', task, 'train', str(i)])\n",
    "    mkdir_if_not_exist(['data/train_valid', task, 'valid', str(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据复制\n",
    "将图片数据复制到各自对应的目录，需要注意的是，这里我们刻意随机打乱了图片的顺序，从而防止训练集与测试集切分不均匀的情况出现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(image_path)\n",
    "random.seed(1024)\n",
    "random.shuffle(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = 0\n",
    "for path, label in image_path:\n",
    "    label_index = list(label).index('y')\n",
    "    if train_count < n * 0.95:\n",
    "        shutil.copy(path,\n",
    "                    os.path.join('data/train_valid', task, 'train', str(label_index)))\n",
    "    else:\n",
    "        shutil.copy(path,\n",
    "                    os.path.join('data/train_valid', task, 'valid', str(label_index)))\n",
    "    train_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法设计——微调版迁移学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 299\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "train_data_dir = os.path.join('data/train_valid', task, 'train')\n",
    "valid_data_dir = os.path.join('data/train_valid', task, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(valid_data_dir,\n",
    "    shuffle=False,\n",
    "    target_size=(image_size, image_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model=InceptionV3(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n",
    "# pretrained_model = ResNet50(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n",
    "# for layer in pretrained_model.layers:\n",
    "#     if re.search(r'^(res5c|bn5c)', layer.name) is not None:\n",
    "#         layer.trainable = True\n",
    "#     else:\n",
    "#          layer.trainable = False\n",
    "\n",
    "for layer in pretrained_model.layers[:140]:\n",
    "    layer.trainable = False\n",
    "for layer in pretrained_model.layers[140:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "x = pretrained_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024)(x)\n",
    "predictions = Dense(task_class, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=pretrained_model.input, outputs=predictions)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pretrained_model.layers:\n",
    "    print(layer.name + \": \" + str(layer.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import * \n",
    "# opt=Adam(1e-5)\n",
    "# opt = SGD(1e-3, momentum=0.9, nesterov=True, decay=1e-5)\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_file_path = 'models/best_model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, early]\n",
    "\n",
    "his = model.fit_generator(train_generator,\n",
    "                            train_generator.n // batch_size, \n",
    "                            epochs=epochs,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=validation_generator.n // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_\"+task+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = his.history['acc'] \n",
    "val_acc = his.history['val_acc'] \n",
    "loss = his.history['loss'] \n",
    "val_loss = his.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc') \n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc') \n",
    "plt.title('Training and validation accuracy') \n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss') \n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss') \n",
    "plt.title('Training and validation loss') \n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
