{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:37:54.342415Z",
     "start_time": "2018-04-06T13:37:54.336251Z"
    }
   },
   "outputs": [],
   "source": [
    "#当采用交叉熵,在train函数中:\n",
    "#_,prediction = torch.max(F.softmax(prediction.data), 1),_代表的是概率值\n",
    "#_,prediction = torch.max(prediction.data, 1),_代表的不是是概率值\n",
    "#注意outputs = model(inputs)的outputs为Variable,而用于torch.max中的应为Tensor\n",
    "#_, preds = torch.max(F.softmax(outputs.data), 1)\n",
    "#局部微调:\n",
    "#model = torchvision.models.resnet18(pretrained=True)\n",
    "#for param in model.parameters():\n",
    "    #param.requires_grad = False\n",
    "#model.fc = nn.Linear(512, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:37:54.426239Z",
     "start_time": "2018-04-06T13:37:54.345267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Author: 孙立波 created on 2018-04-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:37:55.742402Z",
     "start_time": "2018-04-06T13:37:54.429253Z"
    }
   },
   "outputs": [],
   "source": [
    "##############################环境设置#######################\n",
    "import torch\n",
    "from torch.autograd import Variable ## torch中自动计算梯度模块\n",
    "import torch.nn as nn # 神经网络模块\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F  #神经网络模块中的常用功能 \n",
    "import torch.multiprocessing as mp\n",
    "from torch import optim\n",
    "from torch.optim import *\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import datasets, models\n",
    "from torchvision.models import *\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()   # interactive mode\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#显示中文字体设置\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"Droid Sans Fallback\"]\n",
    "plt.rcParams['axes.unicode_minus'] = False #为了正常显示是\"-\"减号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:37:55.753056Z",
     "start_time": "2018-04-06T13:37:55.744528Z"
    }
   },
   "outputs": [],
   "source": [
    "############################## 准备 #######################\n",
    "# MODELS = {\"resnet18\":resnet18,\"resnet34\":resnet34,\"resnet50\":resnet50,\n",
    "#           \"resnet101\":resnet101,\"resnet152\":resnet152,\n",
    "#           \"inception_v3\":inception_v3,\n",
    "#           \"densenet161\":densenet161,\"densenet121\":densenet121,\"densenet169\":densenet169,\"densenet201\":densenet201,\n",
    "#           \"alexnet\":alexnet,\"vgg11\":vgg11,\"vgg11_bn\":vgg11_bn,\"vgg13\":vgg13,\n",
    "#           \"vgg13_bn\":vgg13_bn,\"vgg16\":vgg16,\"vgg16_bn\":vgg16_bn,\"vgg19\":vgg19,\n",
    "#           \"vgg19_bn\":vgg19_bn}\n",
    "#MODELS = {\"resnet18\":resnet18,\"resnet152\":resnet152,\"resnet50\":resnet50,\"inception_v3\":inception_v3,\"densenet161\":densenet161}\n",
    "MODELS = {\"resnet50\":resnet50,\"inception_v3\":inception_v3,\"densenet161\":densenet161} #densenet161最后一层叫classifier,而不叫fc\n",
    "OPTIMIZERS = {\"SGD\":SGD,\"ASGD\":ASGD,\"Adam\":Adam,\"Adagrad\":Adagrad}\n",
    "# classes = [ 'collar_design_labels','neckline_design_labels','skirt_length_labels', \n",
    "#             'sleeve_length_labels', 'neck_design_labels', 'lapel_design_labels', \n",
    "#             'pant_length_labels','coat_length_labels']   \n",
    "#classes = ['collar_design_labels', 'neckline_design_labels', 'neck_design_labels'] \n",
    "classes = ['collar_design_labels'] \n",
    "label_count = {'coat_length_labels':8,\n",
    "               'collar_design_labels':5, \n",
    "               'lapel_design_labels':5,\n",
    "               'neck_design_labels':5,\n",
    "               'neckline_design_labels':10,\n",
    "               'pant_length_labels':6, \n",
    "               'skirt_length_labels':6, \n",
    "               'sleeve_length_labels':9}\n",
    "\n",
    "attrs_cls_label_map = {\n",
    "    'skirt_length_labels':['群不可见Invisible', '短群Short', '及膝群Knee', '旗袍裙群Midi', '及脚群Ankle', '接地群Floor'],\n",
    "    'coat_length_labels': ['衣不可见Invisible','高腰衣HighWaistLength','常规衣RegularLength','长衣LongLength','加长衣MicroLength',\n",
    "                           '及膝衣Knee Length','旗袍衣MidiLength','及地衣Ankle&FloorLength'],\n",
    "    'collar_design_labels': ['衣领不可见Invisible','衬衫领ShirtCollar','彼得潘女士小圆领PeterPan','清道夫领PuritanCollar','螺纹领RibCollar'],\n",
    "    'lapel_design_labels':['翻领不可见Invisible','缺口领Notched','无领Collarless','披肩围巾式领ShawlCollar','大号披肩围巾式领PlusSizeShawl'],\n",
    "    'neck_design_labels':['脖颈不可见Invisible','长高领TurtleNeck', '荷叶半高领RuffleSemi-HighCollar','低圆领LowTurtleNeck','翻领Draped Collar'],\n",
    "    'neckline_design_labels':['颈领线不可见Invisible','无肩带领StraplessNeck','深V领DeepVNeckline', '直领StraightNeck', 'V领VNeckline', \n",
    "                              '方领SquareNeckline', '出肩领OffShoulder', '圆领RoundNeckline', '桃形领SweatHeartNeck', '单肩领OneShoulderNeckline'],\n",
    "    'pant_length_labels':[ '裤不可见Invisible', '短裤ShortPant', '中裤Mid Length', '7分裤3/4Length', '9分裤CroppedPant', '长裤FullLength'],\n",
    "    'sleeve_length_labels':['袖不可见Invisible', '无袖Sleeveless', '杯袖CupSleeves', '短袖ShortSleeves', '肘中袖ElbowSleeves',\n",
    "                            '7分袖Sleeves', '及腕9分袖WristLength', '长袖LongSleeves', '超长袖ExtraLongSleeves']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:37:55.932441Z",
     "start_time": "2018-04-06T13:37:55.754883Z"
    }
   },
   "outputs": [],
   "source": [
    "############################## 参数设置 #######################\n",
    "\n",
    "#参数设置\n",
    "\n",
    "#设置可见的GPU数,注意不是并行训练的设置\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "#若gpu可用则返回True\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "#保存文件后缀,即月份和日为版本尾号\n",
    "#version = \"\"\n",
    "version = '0406augment及选超参和模型'\n",
    "image_width = 224\n",
    "epochs_num = 50\n",
    "scheduler_step_size = 10#设置含参变量的学习率变化，为多少个epoch做一次步进的衰减\n",
    "learning_rate = 0.001\n",
    "split_ratio = 0.2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:37:56.023788Z",
     "start_time": "2018-04-06T13:37:55.935810Z"
    }
   },
   "outputs": [],
   "source": [
    "########################### 定义数据集类和预处理类及操作和数据集实例化测试 #######################\n",
    "\n",
    "######################################################################################\n",
    "#Data augmentation and normalization for training,Just normalization for validation\n",
    "\n",
    "#Transforms on PIL Image\n",
    "#过时class torchvision.transforms.Scale(size, interpolation=2) #按照规定的尺寸重新调节PIL.Image\n",
    "#class torchvision.transforms.Resize(size, interpolation=2)\n",
    "#class torchvision.transforms.CenterCrop(size) #将给定的PIL.Image进行中心切割，得到给定的size，size可以是tuple，(target_height, target_width)。size也可以是一个Integer，在这种情况下，切出来的图片的形状是正方形\n",
    "#class torchvision.transforms.RandomCrop(size, padding=0) #切割中心点的位置随机选取。size可以是tuple也可以是Integer。\n",
    "#class torchvision.transforms.RandomHorizontalFlip(p=0.5) #随机水平翻转给定的PIL.Image,概率为0.5。即：一半的概率翻转，一半的概率不翻转。\n",
    "#过时class torchvision.transforms.RandomSizedCrop(size, interpolation=2) #先将给定的PIL.Image随机切，然后再resize成给定的size大小。\n",
    "#class torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)\n",
    "#class torchvision.transforms.FiveCrop(size) #可能不匹配batchsize，见官网。Crop the given PIL Image into four corners and the central crop\n",
    "#class torchvision.transforms.TenCrop(size, vertical_flip=False)\n",
    "#class torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0) #Randomly change the brightness, contrast and saturation of an image.\n",
    "#class torchvision.transforms.RandomRotation(degrees, resample=False, expand=False, center=None)\n",
    "#class torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)\n",
    "\n",
    "#Transforms on torch.*Tensor\n",
    "#class torchvision.transforms.Normalize(mean, std) #给定均值：(R,G,B) 方差：（R，G，B），将会把Tensor的每个通道矩阵值规范化到正态分布上。即：Normalized_image=(image-mean)/std。\n",
    "\n",
    "#Conversion Transforms\n",
    "#class torchvision.transforms.ToTensor #Convert a PIL Image or numpy.ndarray to tensor.如把一个取值范围是[0,255]的PIL.Image或者shape为(H,W,C)的numpy.ndarray，转换成形状为[C,H,W]，取值范围是[0.0,1.0]的torch.FloadTensor\n",
    "#class torchvision.transforms.ToPILImage(mode=None)\n",
    "\n",
    "#class torchvision.transforms.Lambda(lambd) #使用转换器\n",
    "#Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops]))\n",
    "\n",
    "#应加入的数据增强：对fai一定不要用transforms.RandomCrop(size, padding=0)\n",
    "#transforms.Resize(image_width, interpolation=2) #一般放缩到224*224，并保持边长比不变\n",
    "#transforms.RandomHorizontalFlip(p=0.65) #这个很有用！！！\n",
    "#transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.2) #是随机的，赋值为增益因子\n",
    "#transforms.RandomRotation(20, resample=False, expand=False, center=None)\n",
    "\n",
    "#可能轻微的影响transforms.RandomResizedCrop(image_width, scale=(0.95, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)\n",
    "#可能会有用，可用于测试阶段transforms.CenterCrop((224,180))，如：transforms.CenterCrop(256，224)#注意不是随机裁剪，即高不变，裁剪的宽变化\n",
    "\n",
    "################################################################################\n",
    "#注：定义的dataset经自定义的transform或库自带的基于PIL的数据变换返回的图像和label都应是Tensor形式，而用库自带的变换，需用PIL读取图像！\n",
    "\n",
    "#定义数据预处理\n",
    "\n",
    "#######################transforms.RandomRotation(10)#改变10度\n",
    "        ###############################transforms.ColorJitter(0.05, 0.05, 0.05, 0.05)#微小抖动\n",
    "fai_data_transforms = {\n",
    "                    'train': transforms.Compose([\n",
    "                        transforms.Resize(image_width, interpolation=2),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ColorJitter(brightness=0.08, contrast=0.08, saturation=0.08, hue=0.08),\n",
    "                        transforms.RandomRotation(15, resample=False, expand=False, center=None),\n",
    "                        transforms.RandomResizedCrop(image_width, scale=(0.95, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
    "                        \n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                    ]),\n",
    "                    'val': transforms.Compose([\n",
    "                        #transforms.Resize(256),\n",
    "                        #transforms.CenterCrop(256,224),\n",
    "                        transforms.Resize(image_width, interpolation=2),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                    ]),\n",
    "                    'test': transforms.Compose([\n",
    "                        #transforms.Resize(256),\n",
    "                        #transforms.CenterCrop(256,224),\n",
    "                        transforms.Resize(image_width, interpolation=2),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "}\n",
    "\n",
    "#定义数据集\n",
    "#当变换中含有 transforms.ToTensor(),处理后为RGB CHW 0-1.0数据，当含有transforms.Normalize()会返回一个分布在(x-mean)/std,这时值的范围就不是0-1.0了\n",
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB').resize((224,224),Image.NEAREST)\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "#定义数据集\n",
    "class FaiTrainDataset(Dataset):\n",
    "    \"\"\"FaiTrainDataset dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, train_csv_path_and_file, train_val_images_root_dir, train =True, split_ratio=0.2,transform=None,target_transform=None,loader=default_loader):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(train_csv_path_and_file)\n",
    "        #self.df_load = df.sample(frac=1).reset_index(drop=True)\n",
    "        self.df_load = self.df.copy()\n",
    "        #df.iloc[np.random.permutation(len(df))]\n",
    "        self.split_ratio = split_ratio\n",
    "        self.cut_idx = int(len(self.df_load)-round(len(self.df_load)*self.split_ratio))#int(round(0.1 * self.df_load.shape[0]))\n",
    "        self.df_train= self.df_load.iloc[:self.cut_idx]\n",
    "        self.df_val = self.df_load.iloc[self.cut_idx:]\n",
    "        \n",
    "        self.df_train_load = self.df_train.copy()\n",
    "        self.df_val_load = self.df_val.copy()\n",
    "        \n",
    "        self.df_train_load.columns = ['image_id', 'class', 'label']\n",
    "        self.df_val_load.columns = ['image_id', 'class', 'label']\n",
    "        \n",
    "        self.df_train_load.reset_index(inplace= True,drop=True)\n",
    "        self.df_val_load.reset_index(inplace= True,drop=True)\n",
    "        \n",
    "        self.train_images = self.df_train_load['image_id'].tolist()\n",
    "        self.train_labels = self.df_train_load['label'].tolist()\n",
    "        \n",
    "        self.val_images = self.df_val_load['image_id'].tolist()\n",
    "        self.val_labels = self.df_val_load['label'].tolist()\n",
    "        \n",
    "        n1=len(self.df_train_load)\n",
    "        n2=len(self.df_val_load)\n",
    "        \n",
    "        #不用转化为one-hot，根据所用的交叉熵形式\n",
    "        #self.train_y = np.zeros((n1, label_count[self.df_train_load['class'][0]]), dtype=np.uint8)\n",
    "        #self.val_y = np.zeros((n2, label_count[self.df_val_load['class'][0]]), dtype=np.uint8)\n",
    "        self.train_y = np.zeros(n1, dtype=np.uint8)\n",
    "        self.val_y = np.zeros(n2, dtype=np.uint8)\n",
    "        \n",
    "        for i in range(n1):\n",
    "            tmp_label1=self.train_labels[i]\n",
    "            self.train_y[i]=tmp_label1.find('y')\n",
    "        for j in range(n2):\n",
    "            tmp_label2=self.val_labels[j]\n",
    "            self.val_y[j]=tmp_label2.find('y')\n",
    "        self.train_data = list(zip(self.train_images,self.train_y))\n",
    "        self.val_data = list(zip(self.val_images,self.val_y))\n",
    "        #print(\"训练集：batch化需要的元组样例是{0}：\".format(self.train_data[0]))\n",
    "        #print(\"验证集：batch化需要的元组样例是{0}：\".format(self.val_data[0]))\n",
    "        \n",
    "        self.train_val_images_root_dir = train_val_images_root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train == True:\n",
    "            return len(self.df_train_load)\n",
    "        else:\n",
    "            return len(self.df_val_load)\n",
    "\n",
    "    def __getitem__(self, index): #最终返回的是Tensor\n",
    "        if self.train == True:\n",
    "            train_image_path, train_label = self.train_data[index]\n",
    "            train_img_name = os.path.join(self.train_val_images_root_dir,train_image_path)\n",
    "            #image = io.imread(img_name) #用的skimage.io,读入为uint8，RGB，HWc图像\n",
    "            train_img = self.loader(train_img_name)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                train_img = self.transform(train_img) #处理后为RGB CHW ，个位整数的数据\n",
    "            if self.target_transform is not None:\n",
    "                train_label = self.target_transform(train_label)\n",
    "            return train_img,train_label\n",
    "        else:\n",
    "            val_image_path, val_label = self.val_data[index]\n",
    "            val_img_name = os.path.join(self.train_val_images_root_dir,val_image_path)\n",
    "            #image = io.imread(img_name) #用的skimage.io,读入为uint8，RGB，HWc图像\n",
    "            val_img = self.loader(val_img_name)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                val_img = self.transform(val_img) ##处理后为RGB CHW ，个位整数的数据\n",
    "            if self.target_transform is not None:\n",
    "                val_label = self.target_transform(val_label)\n",
    "            return val_img,val_label #返回值是Tensor\n",
    "            \n",
    "#定义测试数据集\n",
    "class FaiTestDataset(Dataset):\n",
    "    \"\"\"FaiTestDataset dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, test_csv_path_and_file, test_images_root_dir, attr, transform=None,target_transform=None,loader=default_loader):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "       \n",
    "        self.df_test = pd.read_csv(test_csv_path_and_file)\n",
    "        #定义各列名称\n",
    "        self.df_test.columns = ['image_id', 'class', 'x']\n",
    "        del self.df_test['x']\n",
    "        self.attr = attr\n",
    "        \n",
    "        self.df_test_load = self.df_test[(self.df_test['class'] == self.attr)].copy()\n",
    "        self.df_test_load.reset_index(inplace= True,drop= True)\n",
    "        \n",
    "        self.test_images = self.df_test_load['image_id'].tolist()\n",
    "        #n=len(self.df_test_load)\n",
    "        #self.test_y = np.zeros((n, label_count[self.attr]), dtype=np.uint8)\n",
    "        #self.test_data = list(zip(self.test_images,self.test_y))\n",
    "        self.test_data = self.test_images\n",
    "        #print(\"测试集（不含label）：batch化需要的元组样例是{0}：\".format(self.test_data[0]))\n",
    "    \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        self.test_images_root_dir = test_images_root_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_test_load)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        test_path = self.test_data[index]\n",
    "        test_img_name = os.path.join(self.test_images_root_dir,test_path)\n",
    "        #test_image = io.imread(test_img_name) 只能用PIL读图像，因为系统要求\n",
    "        test_img = self.loader(test_img_name)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            test_img = self.transform(test_img) #处理后为RGB CHW ，个位整数的数据\n",
    "\n",
    "        return test_img #返回值为Tensor\n",
    "\n",
    "\n",
    "\n",
    "#测试定义的数据,一个batch的数据可视化\n",
    "def fai_augment_visualize(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0)) #若用cv显示则是：inp = inp.numpy().transpose((1, 2, 0))*255 \n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T13:37:56.148523Z",
     "start_time": "2018-04-06T13:37:56.027207Z"
    }
   },
   "outputs": [],
   "source": [
    "########################### 定义训练和测试及其他可视化函数 #######################\n",
    "#定义训练模型的具体步骤及参数设置\n",
    "def fai_train_model(model, criterion, optimizer, scheduler, batch_size,split_ratio,num_epochs,attr,model_key,version):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    #测试训练，验证和测试数据上的数据增强\n",
    "    training_fai_train_dataset = FaiTrainDataset(train_csv_path_and_file='../train/Annotations/{0}.csv'.format(attr),\n",
    "                                       train_val_images_root_dir='../train/',\n",
    "                                       train =True, split_ratio=split_ratio,\n",
    "                                       transform= fai_data_transforms['train'])\n",
    "    training_fai_val_dataset = FaiTrainDataset(train_csv_path_and_file='../train/Annotations/{0}.csv'.format(attr),\n",
    "                                       train_val_images_root_dir='../train/',\n",
    "                                       train =False, split_ratio=split_ratio,\n",
    "                                       transform= fai_data_transforms['val'])\n",
    "    training_image_datasets = {'train': training_fai_train_dataset, 'val':training_fai_val_dataset}\n",
    "\n",
    "    #关于训练验证集的封装\n",
    "    training_dataloaders = {x: torch.utils.data.DataLoader(training_image_datasets[x], batch_size=batch_size,shuffle=True, num_workers=8)\n",
    "                   for x in ['train', 'val']}\n",
    "    training_dataset_lengths = {x: len(training_image_datasets[x]) for x in ['train', 'val']}\n",
    "    print(\"摘要：Attr:{0}.Train样本数：{1} ，Val样本数：{2}\".format(attr,training_dataset_lengths['train'],training_dataset_lengths['val']))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            #每batch一次train，就val一次\n",
    "            if phase == 'train':\n",
    "                #与Optimizer类似的是，其主要功能体现在step()方法中，用于更新optimizer对象每个param_group字典的lr键的值。\n",
    "                #scheduler = torch.optim.ReduceLROnPlateau(optimizer, 'min')，这应用在函数体外\n",
    "                #scheduler.step() #\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in training_dataloaders[phase]:\n",
    "                # get the inputs(每次一个batch)\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    \n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.long().cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs,volatile=True), Variable(labels,volatile=True)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                #_, preds = torch.max(F.softmax(utputs.data), 1)\n",
    "                _, preds = torch.max(outputs.data, 1) #第一个是最大值的张量(注意此处不是概率值)，第二个是索引值\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                # 即用的总loss而不是平均loss\n",
    "                running_loss += loss.data[0] * inputs.size(0) #inputs.size(0)：每个batchsize的大小\n",
    "                running_corrects += torch.sum(preds == labels.data) #可知标签不用转化为one-hot\n",
    "               \n",
    "            epoch_loss = running_loss / training_dataset_lengths[phase]\n",
    "            epoch_acc = running_corrects / training_dataset_lengths[phase]\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_acc)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    #保存最好的模型\n",
    "    print(\"开始保存模型\")\n",
    "    prefix_cls = attr.split('_')[0]\n",
    "    PATH = '../models/{0}/pytorch_{0}_{1}_{2}'.format(prefix_cls, model_key, version)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    #model = 你定义的模型class封装\n",
    "    #model.load_state_dict(torch.load(PATH))\n",
    "    print(\"保存最好的模型完成\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "######这是将训练好的模型用于测试集上，输出对应属性的也测结果到csv文件\n",
    "#CPU版本的函数\n",
    "def fai_predict(predictor,attr,model_key,version): \n",
    "    fai_test_dataset = FaiTestDataset(test_csv_path_and_file='../test/Tests/question.csv',\n",
    "                                      test_images_root_dir='../test/',\n",
    "                                      attr =attr,\n",
    "                                      transform= fai_data_transforms['test'])\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(fai_test_dataset, batch_size=32,shuffle=False, num_workers=4)\n",
    "    \n",
    "    model = predictor.cpu()\n",
    "    model.train(False)\n",
    "    model.eval() #model.train(False)  # Set model to evaluate mode\n",
    "    #len(test_dataloader)返回的是len(相应dataset)/batch_size\n",
    "    #for i_batch in tqdm(range(len(test_dataloader))):\n",
    "    #在测试集上预测并保存结果\n",
    "    df_test_load = fai_test_dataset.df_test_load\n",
    "    print(\"测试属性：Attr:{0}\".format(attr))\n",
    "    print('测试数据集的样本数为：{0},迭代器需要的迭代次数是：{1}次batchsize的迭代'.format(len(fai_test_dataset),len(test_dataloader)))\n",
    "    \n",
    "    result = []\n",
    "    prefix_cls = attr.split('_')[0]\n",
    "    for i,batch_x in enumerate(test_dataloader):\n",
    "        batch_x = Variable(batch_x, volatile=True)\n",
    "        out = model(batch_x)\n",
    "        out = F.softmax(out,dim=1) #把输出的正负数转到0-1之间\n",
    "        test_np=out.data.numpy()\n",
    "        \n",
    "        #tmp_list = test_np.tolist()\n",
    "        for jj in test_np:\n",
    "            \n",
    "            tmp_result = ''\n",
    "            for tmp_ret in jj:\n",
    "                tmp_result += '{:.5f};'.format(tmp_ret)\n",
    "                #不要最后一个分号\n",
    "            result.append(tmp_result[:-1])\n",
    "    \n",
    "    #预测结果导入内存表格的result列\n",
    "    df_test_load['result'] = result     \n",
    "    df_test_load.to_csv('../result/pytorch/pytorch_{0}_{1}_{2}.csv'.format(prefix_cls, model_key, version), header=None, index=False) \n",
    "    print('#######完成{0}:{1}下的测试集上的csv文件输出'.format(model_key,attr))\n",
    "        \n",
    "######这是可视化预测输出函数，用于测试集上的预测可视化，测试一个batch的输出（设置batchsize大小的是n_pictures）\n",
    "#predictor：模型分类器#attr：要测试的属性#n_pictures：要测试的图片数,最好别大于8,因为输出为1*n的形式\n",
    "#CPU+cuda版本的函数\n",
    "def fai_predict_test_data_visualize(predictor, attr, n_pictures=8,use_gpu= False):\n",
    "    fai_test_dataset = FaiTestDataset(test_csv_path_and_file='../test/Tests/question.csv',\n",
    "                                  test_images_root_dir='../test/',\n",
    "                                  attr =attr,\n",
    "                                  transform= fai_data_transforms['test'])\n",
    "    test_dataloader = torch.utils.data.DataLoader(fai_test_dataset, batch_size=n_pictures,shuffle=False, num_workers=4)\n",
    "    test_dataset_length = len(fai_test_dataset) #注：len(test_dataloader)返回的是len(相应dataset)/batch_size\n",
    "    if use_gpu:\n",
    "        model = predictor.cuda()\n",
    "        model.train(False)\n",
    "        model.eval()\n",
    "    else:\n",
    "        \n",
    "        model = predictor.cpu()\n",
    "        model.train(False)\n",
    "        model.eval()\n",
    "    # get some random training images\n",
    "    dataiter = iter(test_dataloader)\n",
    "    batch_x= dataiter.next()\n",
    "    if use_gpu:\n",
    "        batch_xx = Variable(batch_x.cuda(), volatile=True)\n",
    "    else:\n",
    "        batch_xx = Variable(batch_x,volatile=True)  \n",
    "    outputs = model(batch_xx)\n",
    "    _, predicted = torch.max(outputs.data, 1) #此时predicted为Tensor类型的索引列表\n",
    "\n",
    "    if use_gpu:\n",
    "        aa = predicted.cpu().numpy()\n",
    "    else:\n",
    "        aa = predicted.numpy()\n",
    "    fig,axes = plt.subplots(ncols=n_pictures,figsize=(4*n_pictures,4))  \n",
    "    print('{0}张图片预测得到的索引:'.format(n_pictures))\n",
    "    print(aa)\n",
    "    for i in range(n_pictures):\n",
    "        \"\"\"Imshow for Tensor.\"\"\"\n",
    "        inp = batch_x[i].numpy().transpose((1, 2, 0)) #若用cv显示则是：inp = inp.numpy().transpose((1, 2, 0))*255 \n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        inp = std * inp + mean\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "        k = aa[i]\n",
    "        axes[i].set_title(\"pred-vis-{0},\\nPred:{1}\".format(attr,attrs_cls_label_map[attr][k]),color='r')  \n",
    "        axes[i].imshow(inp)\n",
    "    print('完成测试集上的一个batchsize测试!')\n",
    "    #plt.savefig('../images/{0}/pytorch_{0}_{1}_{2}.png'.format(prefix_cls, KEY, version),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T16:20:47.837155Z",
     "start_time": "2018-04-06T13:37:56.151580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################在resnet50下训练8个分类器####################\n",
      "\n",
      "######################在resnet50:SGD下训练8个分类器####################\n",
      "\n",
      "#######resnet50:SGD:collar_design_labels####################\n",
      "摘要：Attr:collar_design_labels.Train样本数：6714 ，Val样本数：1678\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.5583 Acc: 0.2928\n",
      "val Loss: 1.4819 Acc: 0.3689\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.4481 Acc: 0.4124\n",
      "val Loss: 1.3659 Acc: 0.4672\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.3399 Acc: 0.4885\n",
      "val Loss: 1.2521 Acc: 0.5316\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.2402 Acc: 0.5307\n",
      "val Loss: 1.1635 Acc: 0.5638\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.1422 Acc: 0.5600\n",
      "val Loss: 1.0780 Acc: 0.5948\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.0680 Acc: 0.5982\n",
      "val Loss: 1.0068 Acc: 0.6234\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.9919 Acc: 0.6230\n",
      "val Loss: 0.9467 Acc: 0.6478\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.9366 Acc: 0.6436\n",
      "val Loss: 0.8937 Acc: 0.6645\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.8810 Acc: 0.6670\n",
      "val Loss: 0.8444 Acc: 0.6806\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.8336 Acc: 0.6953\n",
      "val Loss: 0.8061 Acc: 0.6985\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.7859 Acc: 0.7082\n",
      "val Loss: 0.7725 Acc: 0.7092\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.7468 Acc: 0.7206\n",
      "val Loss: 0.7415 Acc: 0.7259\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.7214 Acc: 0.7355\n",
      "val Loss: 0.7228 Acc: 0.7324\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.6762 Acc: 0.7513\n",
      "val Loss: 0.7044 Acc: 0.7426\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.6646 Acc: 0.7535\n",
      "val Loss: 0.6884 Acc: 0.7491\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.6280 Acc: 0.7679\n",
      "val Loss: 0.6724 Acc: 0.7503\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.5995 Acc: 0.7764\n",
      "val Loss: 0.6617 Acc: 0.7616\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.5809 Acc: 0.7887\n",
      "val Loss: 0.6522 Acc: 0.7664\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.5667 Acc: 0.7943\n",
      "val Loss: 0.6428 Acc: 0.7712\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.5425 Acc: 0.8007\n",
      "val Loss: 0.6376 Acc: 0.7735\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.5200 Acc: 0.8108\n",
      "val Loss: 0.6310 Acc: 0.7777\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.4998 Acc: 0.8192\n",
      "val Loss: 0.6266 Acc: 0.7843\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4849 Acc: 0.8205\n",
      "val Loss: 0.6222 Acc: 0.7813\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.4662 Acc: 0.8271\n",
      "val Loss: 0.6139 Acc: 0.7825\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.4599 Acc: 0.8301\n",
      "val Loss: 0.6151 Acc: 0.7729\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.4459 Acc: 0.8381\n",
      "val Loss: 0.6119 Acc: 0.7831\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.4183 Acc: 0.8545\n",
      "val Loss: 0.6138 Acc: 0.7878\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.4030 Acc: 0.8551\n",
      "val Loss: 0.6146 Acc: 0.7837\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.3893 Acc: 0.8604\n",
      "val Loss: 0.6062 Acc: 0.7849\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.3839 Acc: 0.8610\n",
      "val Loss: 0.6113 Acc: 0.7950\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.3699 Acc: 0.8697\n",
      "val Loss: 0.6122 Acc: 0.7902\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3490 Acc: 0.8709\n",
      "val Loss: 0.6152 Acc: 0.7914\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.3318 Acc: 0.8816\n",
      "val Loss: 0.6217 Acc: 0.7878\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.3198 Acc: 0.8858\n",
      "val Loss: 0.6228 Acc: 0.7896\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.3107 Acc: 0.8887\n",
      "val Loss: 0.6257 Acc: 0.7843\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.2935 Acc: 0.8950\n",
      "val Loss: 0.6275 Acc: 0.7884\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.2801 Acc: 0.9038\n",
      "val Loss: 0.6313 Acc: 0.7890\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.2762 Acc: 0.9029\n",
      "val Loss: 0.6503 Acc: 0.7807\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.2453 Acc: 0.9153\n",
      "val Loss: 0.6463 Acc: 0.7908\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2489 Acc: 0.9139\n",
      "val Loss: 0.6395 Acc: 0.7944\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.2420 Acc: 0.9111\n",
      "val Loss: 0.6367 Acc: 0.7926\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.2159 Acc: 0.9327\n",
      "val Loss: 0.6397 Acc: 0.7938\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.2308 Acc: 0.9211\n",
      "val Loss: 0.6452 Acc: 0.7914\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.2192 Acc: 0.9237\n",
      "val Loss: 0.6405 Acc: 0.7908\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.2261 Acc: 0.9206\n",
      "val Loss: 0.6387 Acc: 0.7950\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.2215 Acc: 0.9254\n",
      "val Loss: 0.6448 Acc: 0.7926\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.2129 Acc: 0.9285\n",
      "val Loss: 0.6424 Acc: 0.7920\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.2265 Acc: 0.9191\n",
      "val Loss: 0.6406 Acc: 0.7944\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9227\n",
      "val Loss: 0.6424 Acc: 0.7956\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.2024 Acc: 0.9336\n",
      "val Loss: 0.6506 Acc: 0.7896\n",
      "\n",
      "Training complete in 34m 34s\n",
      "Best val Acc: 0.795590\n",
      "开始保存模型\n",
      "保存最好的模型完成\n",
      "#######resnet50:collar_design_labels训练完毕，开始在测试集上测试\n",
      "测试属性：Attr:collar_design_labels\n",
      "测试数据集的样本数为：1081,迭代器需要的迭代次数是：34次batchsize的迭代\n",
      "#######完成resnet50:collar_design_labels下的测试集上的csv文件输出\n",
      "######################在resnet50:ASGD下训练8个分类器####################\n",
      "\n",
      "#######resnet50:ASGD:collar_design_labels####################\n",
      "摘要：Attr:collar_design_labels.Train样本数：6714 ，Val样本数：1678\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.5588 Acc: 0.3043\n",
      "val Loss: 1.4715 Acc: 0.4160\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.4444 Acc: 0.4114\n",
      "val Loss: 1.3540 Acc: 0.4744\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.3276 Acc: 0.4945\n",
      "val Loss: 1.2334 Acc: 0.5322\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 1.2144 Acc: 0.5365\n",
      "val Loss: 1.1340 Acc: 0.5620\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 1.1175 Acc: 0.5770\n",
      "val Loss: 1.0341 Acc: 0.6067\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.0342 Acc: 0.6122\n",
      "val Loss: 0.9745 Acc: 0.6263\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.9612 Acc: 0.6402\n",
      "val Loss: 0.9112 Acc: 0.6615\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.9043 Acc: 0.6591\n",
      "val Loss: 0.8543 Acc: 0.6830\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.8455 Acc: 0.6881\n",
      "val Loss: 0.8137 Acc: 0.6996\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.7988 Acc: 0.7014\n",
      "val Loss: 0.7788 Acc: 0.7199\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.7711 Acc: 0.7145\n",
      "val Loss: 0.7498 Acc: 0.7241\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.7290 Acc: 0.7277\n",
      "val Loss: 0.7300 Acc: 0.7342\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.7015 Acc: 0.7389\n",
      "val Loss: 0.7085 Acc: 0.7414\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.6686 Acc: 0.7516\n",
      "val Loss: 0.6896 Acc: 0.7479\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.6427 Acc: 0.7660\n",
      "val Loss: 0.6705 Acc: 0.7586\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.6058 Acc: 0.7761\n",
      "val Loss: 0.6572 Acc: 0.7616\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.5902 Acc: 0.7790\n",
      "val Loss: 0.6533 Acc: 0.7646\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.5650 Acc: 0.7925\n",
      "val Loss: 0.6413 Acc: 0.7718\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.5400 Acc: 0.8019\n",
      "val Loss: 0.6345 Acc: 0.7706\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.5251 Acc: 0.8021\n",
      "val Loss: 0.6334 Acc: 0.7753\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.5084 Acc: 0.8111\n",
      "val Loss: 0.6177 Acc: 0.7783\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.4898 Acc: 0.8217\n",
      "val Loss: 0.6133 Acc: 0.7789\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4826 Acc: 0.8229\n",
      "val Loss: 0.6204 Acc: 0.7795\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.4726 Acc: 0.8295\n",
      "val Loss: 0.6162 Acc: 0.7831\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.4499 Acc: 0.8399\n",
      "val Loss: 0.6149 Acc: 0.7861\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.4273 Acc: 0.8454\n",
      "val Loss: 0.6075 Acc: 0.7789\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.4027 Acc: 0.8512\n",
      "val Loss: 0.6097 Acc: 0.7867\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.3831 Acc: 0.8625\n",
      "val Loss: 0.6081 Acc: 0.7926\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.3802 Acc: 0.8612\n",
      "val Loss: 0.6030 Acc: 0.7926\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.3625 Acc: 0.8668\n",
      "val Loss: 0.6110 Acc: 0.7837\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.3536 Acc: 0.8728\n",
      "val Loss: 0.6173 Acc: 0.7896\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.3479 Acc: 0.8730\n",
      "val Loss: 0.6068 Acc: 0.7956\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.3297 Acc: 0.8805\n",
      "val Loss: 0.6083 Acc: 0.7902\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.3098 Acc: 0.8875\n",
      "val Loss: 0.6152 Acc: 0.7807\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.2987 Acc: 0.8917\n",
      "val Loss: 0.6180 Acc: 0.7783\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.2909 Acc: 0.8965\n",
      "val Loss: 0.6310 Acc: 0.7944\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.2807 Acc: 0.8999\n",
      "val Loss: 0.6241 Acc: 0.7932\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.2746 Acc: 0.9014\n",
      "val Loss: 0.6228 Acc: 0.7980\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.2461 Acc: 0.9167\n",
      "val Loss: 0.6281 Acc: 0.7950\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.2512 Acc: 0.9130\n",
      "val Loss: 0.6528 Acc: 0.7908\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2463 Acc: 0.9123\n",
      "val Loss: 0.6418 Acc: 0.7926\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9251\n",
      "val Loss: 0.6436 Acc: 0.7932\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.2258 Acc: 0.9234\n",
      "val Loss: 0.6553 Acc: 0.7968\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.2107 Acc: 0.9282\n",
      "val Loss: 0.6557 Acc: 0.7926\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.1922 Acc: 0.9376\n",
      "val Loss: 0.6670 Acc: 0.7956\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.1845 Acc: 0.9373\n",
      "val Loss: 0.6622 Acc: 0.7902\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.1865 Acc: 0.9348\n",
      "val Loss: 0.6773 Acc: 0.7938\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.1750 Acc: 0.9424\n",
      "val Loss: 0.6936 Acc: 0.7932\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.9501\n",
      "val Loss: 0.6901 Acc: 0.7992\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.1620 Acc: 0.9453\n",
      "val Loss: 0.6983 Acc: 0.7872\n",
      "\n",
      "Training complete in 34m 33s\n",
      "Best val Acc: 0.799166\n",
      "开始保存模型\n",
      "保存最好的模型完成\n",
      "#######resnet50:collar_design_labels训练完毕，开始在测试集上测试\n",
      "测试属性：Attr:collar_design_labels\n",
      "测试数据集的样本数为：1081,迭代器需要的迭代次数是：34次batchsize的迭代\n",
      "#######完成resnet50:collar_design_labels下的测试集上的csv文件输出\n",
      "######################在resnet50:Adam下训练8个分类器####################\n",
      "\n",
      "#######resnet50:Adam:collar_design_labels####################\n",
      "摘要：Attr:collar_design_labels.Train样本数：6714 ，Val样本数：1678\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.4712 Acc: 0.3658\n",
      "val Loss: 1.3646 Acc: 0.4362\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.2184 Acc: 0.5030\n",
      "val Loss: 1.1030 Acc: 0.5393\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.1077 Acc: 0.5634\n",
      "val Loss: 1.0234 Acc: 0.5995\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.9979 Acc: 0.6141\n",
      "val Loss: 0.9737 Acc: 0.6311\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.9428 Acc: 0.6366\n",
      "val Loss: 0.8861 Acc: 0.6788\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.8745 Acc: 0.6662\n",
      "val Loss: 0.9013 Acc: 0.6597\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.8231 Acc: 0.6950\n",
      "val Loss: 0.8291 Acc: 0.6877\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.7834 Acc: 0.7078\n",
      "val Loss: 0.8023 Acc: 0.6913\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.7565 Acc: 0.7121\n",
      "val Loss: 0.9261 Acc: 0.6788\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.7054 Acc: 0.7392\n",
      "val Loss: 0.8379 Acc: 0.6925\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.7050 Acc: 0.7407\n",
      "val Loss: 0.7606 Acc: 0.7205\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.6672 Acc: 0.7542\n",
      "val Loss: 0.7290 Acc: 0.7384\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.6375 Acc: 0.7699\n",
      "val Loss: 0.8000 Acc: 0.7086\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.6412 Acc: 0.7590\n",
      "val Loss: 0.8315 Acc: 0.7026\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.5990 Acc: 0.7845\n",
      "val Loss: 0.7768 Acc: 0.7253\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.5883 Acc: 0.7819\n",
      "val Loss: 0.7651 Acc: 0.7181\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.5653 Acc: 0.7951\n",
      "val Loss: 0.6457 Acc: 0.7610\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.5471 Acc: 0.8003\n",
      "val Loss: 0.6761 Acc: 0.7485\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.5373 Acc: 0.8068\n",
      "val Loss: 0.6920 Acc: 0.7628\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.5140 Acc: 0.8123\n",
      "val Loss: 0.6609 Acc: 0.7741\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.4926 Acc: 0.8226\n",
      "val Loss: 0.7362 Acc: 0.7414\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.4697 Acc: 0.8299\n",
      "val Loss: 0.7268 Acc: 0.7420\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.4674 Acc: 0.8290\n",
      "val Loss: 0.7240 Acc: 0.7569\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.4436 Acc: 0.8374\n",
      "val Loss: 0.7286 Acc: 0.7557\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.4416 Acc: 0.8418\n",
      "val Loss: 0.6231 Acc: 0.7729\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.4236 Acc: 0.8418\n",
      "val Loss: 0.7208 Acc: 0.7616\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.4072 Acc: 0.8531\n",
      "val Loss: 0.7167 Acc: 0.7682\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.3942 Acc: 0.8558\n",
      "val Loss: 0.8219 Acc: 0.7378\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.3955 Acc: 0.8546\n",
      "val Loss: 0.7368 Acc: 0.7449\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.3574 Acc: 0.8744\n",
      "val Loss: 0.7368 Acc: 0.7503\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.3715 Acc: 0.8657\n",
      "val Loss: 0.8056 Acc: 0.7557\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.2648 Acc: 0.9044\n",
      "val Loss: 0.6226 Acc: 0.7902\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.2175 Acc: 0.9237\n",
      "val Loss: 0.6290 Acc: 0.8027\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.1911 Acc: 0.9331\n",
      "val Loss: 0.6556 Acc: 0.7974\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.1764 Acc: 0.9371\n",
      "val Loss: 0.6648 Acc: 0.7968\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.1634 Acc: 0.9415\n",
      "val Loss: 0.6717 Acc: 0.8033\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.1425 Acc: 0.9511\n",
      "val Loss: 0.6858 Acc: 0.8027\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.1414 Acc: 0.9474\n",
      "val Loss: 0.7381 Acc: 0.8004\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.1396 Acc: 0.9500\n",
      "val Loss: 0.7264 Acc: 0.8027\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.1270 Acc: 0.9546\n",
      "val Loss: 0.7380 Acc: 0.7968\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.1212 Acc: 0.9576\n",
      "val Loss: 0.7756 Acc: 0.7944\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.1115 Acc: 0.9607\n",
      "val Loss: 0.7998 Acc: 0.7968\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.1100 Acc: 0.9619\n",
      "val Loss: 0.7980 Acc: 0.7944\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.1092 Acc: 0.9616\n",
      "val Loss: 0.7786 Acc: 0.8075\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.1078 Acc: 0.9628\n",
      "val Loss: 0.7867 Acc: 0.7974\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0896 Acc: 0.9701\n",
      "val Loss: 0.8440 Acc: 0.7986\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.9654\n",
      "val Loss: 0.8197 Acc: 0.8015\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0834 Acc: 0.9684\n",
      "val Loss: 0.8432 Acc: 0.7944\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0843 Acc: 0.9693\n",
      "val Loss: 0.8326 Acc: 0.7974\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0820 Acc: 0.9699\n",
      "val Loss: 0.8843 Acc: 0.7974\n",
      "\n",
      "Training complete in 35m 0s\n",
      "Best val Acc: 0.807509\n",
      "开始保存模型\n",
      "保存最好的模型完成\n",
      "#######resnet50:collar_design_labels训练完毕，开始在测试集上测试\n",
      "测试属性：Attr:collar_design_labels\n",
      "测试数据集的样本数为：1081,迭代器需要的迭代次数是：34次batchsize的迭代\n",
      "#######完成resnet50:collar_design_labels下的测试集上的csv文件输出\n",
      "######################在resnet50:Adagrad下训练8个分类器####################\n",
      "\n",
      "#######resnet50:Adagrad:collar_design_labels####################\n",
      "摘要：Attr:collar_design_labels.Train样本数：6714 ，Val样本数：1678\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.9729 Acc: 0.6163\n",
      "val Loss: 0.6612 Acc: 0.7509\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.5973 Acc: 0.7809\n",
      "val Loss: 0.5736 Acc: 0.7855\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.4376 Acc: 0.8385\n",
      "val Loss: 0.6057 Acc: 0.7801\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.3520 Acc: 0.8756\n",
      "val Loss: 0.5959 Acc: 0.7998\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.2795 Acc: 0.9007\n",
      "val Loss: 0.6175 Acc: 0.8033\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.2206 Acc: 0.9249\n",
      "val Loss: 0.7045 Acc: 0.7902\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.1780 Acc: 0.9377\n",
      "val Loss: 0.6307 Acc: 0.8123\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.1466 Acc: 0.9500\n",
      "val Loss: 0.6647 Acc: 0.8093\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.1175 Acc: 0.9595\n",
      "val Loss: 0.6456 Acc: 0.8081\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0916 Acc: 0.9707\n",
      "val Loss: 0.7162 Acc: 0.8021\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.0704 Acc: 0.9775\n",
      "val Loss: 0.7326 Acc: 0.8111\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0710 Acc: 0.9778\n",
      "val Loss: 0.7025 Acc: 0.8135\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0674 Acc: 0.9794\n",
      "val Loss: 0.7020 Acc: 0.8075\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0565 Acc: 0.9830\n",
      "val Loss: 0.7071 Acc: 0.8147\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0544 Acc: 0.9839\n",
      "val Loss: 0.7028 Acc: 0.8111\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0436 Acc: 0.9879\n",
      "val Loss: 0.7021 Acc: 0.8182\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0424 Acc: 0.9884\n",
      "val Loss: 0.7729 Acc: 0.8105\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0331 Acc: 0.9915\n",
      "val Loss: 0.7334 Acc: 0.8135\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0289 Acc: 0.9920\n",
      "val Loss: 0.7554 Acc: 0.8212\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0282 Acc: 0.9924\n",
      "val Loss: 0.7710 Acc: 0.8111\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.0242 Acc: 0.9933\n",
      "val Loss: 0.7599 Acc: 0.8135\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0274 Acc: 0.9923\n",
      "val Loss: 0.7864 Acc: 0.8123\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0240 Acc: 0.9930\n",
      "val Loss: 0.7684 Acc: 0.8069\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0217 Acc: 0.9948\n",
      "val Loss: 0.7844 Acc: 0.8147\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0237 Acc: 0.9940\n",
      "val Loss: 0.7802 Acc: 0.8105\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0177 Acc: 0.9963\n",
      "val Loss: 0.7750 Acc: 0.8147\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0162 Acc: 0.9957\n",
      "val Loss: 0.8175 Acc: 0.8093\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0145 Acc: 0.9963\n",
      "val Loss: 0.8235 Acc: 0.8176\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0182 Acc: 0.9954\n",
      "val Loss: 0.8349 Acc: 0.8075\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0165 Acc: 0.9958\n",
      "val Loss: 0.8240 Acc: 0.8093\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.0164 Acc: 0.9957\n",
      "val Loss: 0.7784 Acc: 0.8153\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0137 Acc: 0.9963\n",
      "val Loss: 0.7884 Acc: 0.8123\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0122 Acc: 0.9972\n",
      "val Loss: 0.7708 Acc: 0.8176\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0155 Acc: 0.9964\n",
      "val Loss: 0.7836 Acc: 0.8182\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0104 Acc: 0.9978\n",
      "val Loss: 0.7860 Acc: 0.8212\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0139 Acc: 0.9969\n",
      "val Loss: 0.7749 Acc: 0.8206\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0159 Acc: 0.9957\n",
      "val Loss: 0.7739 Acc: 0.8212\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0100 Acc: 0.9976\n",
      "val Loss: 0.7773 Acc: 0.8242\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0129 Acc: 0.9970\n",
      "val Loss: 0.7788 Acc: 0.8230\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0090 Acc: 0.9975\n",
      "val Loss: 0.7810 Acc: 0.8248\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.0123 Acc: 0.9975\n",
      "val Loss: 0.7705 Acc: 0.8206\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0103 Acc: 0.9978\n",
      "val Loss: 0.7788 Acc: 0.8218\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0106 Acc: 0.9970\n",
      "val Loss: 0.7934 Acc: 0.8212\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0115 Acc: 0.9976\n",
      "val Loss: 0.7790 Acc: 0.8188\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0120 Acc: 0.9975\n",
      "val Loss: 0.7898 Acc: 0.8236\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0112 Acc: 0.9975\n",
      "val Loss: 0.7717 Acc: 0.8200\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0093 Acc: 0.9975\n",
      "val Loss: 0.7853 Acc: 0.8212\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0105 Acc: 0.9975\n",
      "val Loss: 0.7940 Acc: 0.8170\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0097 Acc: 0.9975\n",
      "val Loss: 0.7811 Acc: 0.8224\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0087 Acc: 0.9987\n",
      "val Loss: 0.7801 Acc: 0.8176\n",
      "\n",
      "Training complete in 34m 55s\n",
      "Best val Acc: 0.824791\n",
      "开始保存模型\n",
      "保存最好的模型完成\n",
      "#######resnet50:collar_design_labels训练完毕，开始在测试集上测试\n",
      "测试属性：Attr:collar_design_labels\n",
      "测试数据集的样本数为：1081,迭代器需要的迭代次数是：34次batchsize的迭代\n",
      "#######完成resnet50:collar_design_labels下的测试集上的csv文件输出\n",
      "######################在inception_v3下训练8个分类器####################\n",
      "\n",
      "######################在inception_v3:SGD下训练8个分类器####################\n",
      "\n",
      "#######inception_v3:SGD:collar_design_labels####################\n",
      "摘要：Attr:collar_design_labels.Train样本数：6714 ，Val样本数：1678\n",
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (128, 3, 3). Calculated output size: (32, -1, -1). Output size is too small.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-027e4314100c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m#训练和评估\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfai_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfai_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfai_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;31m#在测试集上运用训练好的模型，输出csv文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#######{0}:{1}训练完毕，开始在测试集上测试'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_class\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4ff4d135f9e6>\u001b[0m in \u001b[0;36mfai_train_model\u001b[0;34m(model, criterion, optimizer, scheduler, batch_size, split_ratio, num_epochs, attr, model_key, version)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0;31m#_, preds = torch.max(F.softmax(utputs.data), 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#第一个是最大值的张量(注意此处不是概率值)，第二个是索引值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# 17 x 17 x 768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAuxLogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m# 17 x 17 x 768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_7a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# 5 x 5 x 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;31m# 1 x 1 x 768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 282\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (128, 3, 3). Calculated output size: (32, -1, -1). Output size is too small."
     ]
    }
   ],
   "source": [
    "################################### 实例化训练和测试 ############################\n",
    "#加载预训练模型并重写全连接层。\n",
    "\n",
    "#state_dict = torch.utils.model_zoo.load_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
    "#the_model = TheModelClass(*args, **kwargs)\n",
    "#the_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#下面是冻结卷积层的方法\n",
    "# fai_model = torchvision.models.resnet18(pretrained=True)\n",
    "# for param in fai_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "#model.fc = nn.Linear(512, 100)\n",
    "# Optimize only the classifier\n",
    "#optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)\n",
    "for KEY, MODLE in MODELS.items():\n",
    "    print('######################在{0}下训练8个分类器####################'.format(KEY))\n",
    "    print()\n",
    "    for KEY2,OPTIMIZER in OPTIMIZERS.items():\n",
    "        print('######################在{0}:{1}下训练8个分类器####################'.format(KEY,KEY2))\n",
    "        print()\n",
    "        for cur_class in classes:\n",
    "\n",
    "            print('#######{0}:{1}:{2}####################'.format(KEY,KEY2, cur_class))\n",
    "            #预结构\n",
    "            fai_model = MODLE(pretrained=True)\n",
    "\n",
    "            #改进的自定义添加\n",
    "            if KEY == 'densenet161':\n",
    "                \n",
    "                num_ftrs = fai_model.classifier.in_features\n",
    "                fai_model.classifier = nn.Linear(num_ftrs, label_count[cur_class])\n",
    "            else:\n",
    "                num_ftrs = fai_model.fc.in_features\n",
    "                fai_model.fc = nn.Linear(num_ftrs, label_count[cur_class])\n",
    "\n",
    "\n",
    "            #打印含参数各层的名字\n",
    "            #params = fai_model.state_dict()\n",
    "            #for k,v in params.items():\n",
    "            #    print(k)#打印网络中的变量名\n",
    "            #print(fai_model)#可查最后一层Module类对象的名和输入参数，输出参数名，如(fc): Linear(in_features=512, out_features=5, bias=True)\n",
    "\n",
    "\n",
    "            if use_gpu:\n",
    "                #If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. \n",
    "                fai_model = fai_model.cuda()\n",
    "\n",
    "            #设置采用多分类的交叉熵loss\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # 设置优化器，fai_model.parameters()表示优化全部参数@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@这里可设置两部优化法\n",
    "            #torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "            #torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0)\n",
    "            #torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "            #torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "            #class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "            #class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "            #torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n",
    "            #torch.optim.SGD(params, lr=<object object>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "            optimizer = OPTIMIZER(fai_model.parameters(), lr=learning_rate)\n",
    "            #optimizer = optim.Adam(ai_model.parameters(), lr = learning_rate)#olearning_rate=0.0001,optimizer = optim.Adam([var1, var2], lr = 0.0001)\n",
    "\n",
    "\n",
    "            # Decay LR by a factor of 0.1 every 7 epochs\n",
    "            #与Optimizer类似的是，其主要功能体现在step()方法中，用于更新optimizer对象每个param_group字典的lr键的值。\n",
    "            #scheduler = torch.optim.ReduceLROnPlateau(optimizer, 'min')   scheduler.step(val_loss)\n",
    "            #fai_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=0.1) \n",
    "            fai_lr_scheduler = ReduceLROnPlateau(optimizer, 'max',patience=10,factor = 0.1,verbose = False)\n",
    "\n",
    "            #训练和评估\n",
    "            model = fai_train_model(fai_model, criterion, optimizer, fai_lr_scheduler, batch_size=batch_size,split_ratio=split_ratio,num_epochs=epochs_num,attr=cur_class,model_key=KEY,version=version)\n",
    "            #在测试集上运用训练好的模型，输出csv文件\n",
    "            print('#######{0}:{1}训练完毕，开始在测试集上测试'.format(KEY,cur_class ))\n",
    "            #在cpu上测试\n",
    "            model = model.cpu()\n",
    "            fai_predict(predictor = model,attr=cur_class,model_key=KEY,version=version)\n",
    "            #fai_predict_test_data_visualize(predictor= model, attr=cur_class, n_pictures=8,use_gpu= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T16:20:47.839176Z",
     "start_time": "2018-04-06T13:37:54.356Z"
    }
   },
   "outputs": [],
   "source": [
    "##############################模型实例部署并进行可视化测试#############################################\n",
    "#自定义载入相应模型来自finetune或者是model来自你定义的模型class封装，并进行可视化测试\n",
    "\n",
    "#下面是finetune示例\n",
    "#KEY = finetune改进的模型类型，在此为对应的字符串\n",
    "#预结构\n",
    "#fai_model = models.MODELS[KEY](pretrained=True)\n",
    "#改进的自定义添加\n",
    "#num_ftrs = fai_model.fc.in_features\n",
    "#fai_model.fc = nn.Linear(num_ftrs, label_count[cur_class])\n",
    "\n",
    "#prefix_cls = classes[0].split('_')[0]\n",
    "#PATH = '../models/{0}/pytorch_{0}_{1}_{2}'.format(prefix_cls, model_key=KEY, version=version)\n",
    "#torch.save(model.state_dict(), PATH)\n",
    "#装入训练好的模型的权重\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "#fai_predict_test_data_visualize(predictor = fai_model, classes[0], n_pictures = batch_size,use_gpu= False):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
